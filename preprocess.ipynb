{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "common-spanish",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "stunning-sperm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# math and data packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# charting and graphics\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# os and file types\n",
    "import os\n",
    "import sys\n",
    "import datetime as dt\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# images and display\n",
    "import base64, io, IPython\n",
    "from PIL import Image as PILImage\n",
    "from IPython.display import Markdown as md\n",
    "from IPython.display import display, Math, Latex\n",
    "\n",
    "\n",
    "# set useful variables for accessing the files\n",
    "here = os.getcwd()\n",
    "flora_h = F\"{here}/resources/\"\n",
    "flora_h_ws = F\"{flora_h}atlasws/\"\n",
    "flora_h_55 = F\"{flora_h}atlas5x5/\"\n",
    "data_2020 = F\"{here}/resources/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "drawn-individual",
   "metadata": {},
   "source": [
    "## Import data\n",
    "\n",
    "Fix any known formatting problems here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "wound-banks",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/mwshovel/dev/collaborate/botany/florabuckets/flora-buckets/resources/data-2021.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9613/2007250287.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;31m# use the get_the_data method to collect these files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0mdata_and_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_the_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_2020\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_data_methods\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthis_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0mwatch_lists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_the_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_lists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflora_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_data_methods\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthis_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_9613/2007250287.py\u001b[0m in \u001b[0;36mget_the_data\u001b[0;34m(file_exts, a_dir, methods, this_method, myencoding)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_exts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmyencoding\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0mwiw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmethods\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mthis_method\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mF\"{a_dir}{v}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mwiw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmethods\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mthis_method\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mF\"{a_dir}{v}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\";\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmyencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/iqaasl/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/iqaasl/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/iqaasl/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/iqaasl/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/iqaasl/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/iqaasl/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \"\"\"\n\u001b[0;32m-> 1362\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/iqaasl/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/mwshovel/dev/collaborate/botany/florabuckets/flora-buckets/resources/data-2021.csv'"
     ]
    }
   ],
   "source": [
    "# importing the data files\n",
    "# create variables for convenience method and grouping\n",
    "\n",
    "# start by organizing them into dictionaries.\n",
    "my_data_methods = {\"csv\":pd.read_csv}\n",
    "\n",
    "# dict for the 2020 survey data files\n",
    "d_files = {\n",
    "    \"surveys_20\":\"surveys.csv\",\n",
    "    \"surveys_21a\":\"data-2021.csv\",\n",
    "    \"surveys_21b\":\"obs_export_2021-10-26_20h46.csv\",\n",
    "    \"map_keys_20\":\"map-keys.csv\",\n",
    "    \"map_keys_21\":\"2021-survey-key.csv\"\n",
    "}\n",
    "# match map_keys_21 to surveys_21A\n",
    "# date and time format correction\n",
    "# species slug\n",
    "# format to 2020\n",
    "\n",
    "# dict for the reference files that are not Welten Sutter or Flora Helvetica 5X5 lists\n",
    "w_lists = {\n",
    "    \"list_2014\":\"BL_WL_2014_modified.csv\",\n",
    "    \"under_sampled\":\"taxa_sous_echantillonnes.csv\",\n",
    "    \"red_list\":\"redlist2019.csv\",\n",
    "    \"cert_list\": \"Certification_specieslist_2021.csv\",\n",
    "    \"ch_priority\":\"ch_priority_species.csv\"\n",
    "}\n",
    "\n",
    "# format and import ch_priority_species.csv to use with 2020 & 2021 survey results\n",
    "# evaluate the survey results with respect to the columns \"priorité\tmenace\tresponsabilité\tnécessité de prendre des mesures\tnécessité de surveiller les populations\tconnaissances suffisantes?\ttechniques connues?\n",
    "# repeat 2020 data with redlist with 2021A\n",
    "\n",
    "v_lists = {\n",
    "    \"imp_seeds\":\"important-seeds-2021.csv\",\n",
    "    \"all_seeds\":\"all-seeds-2021.csv\",\n",
    "    \"inv_cave\":\"volo-inventory-de.csv\",\n",
    "    \"germ_exp\":\"germination-experiment.csv\"\n",
    "    \n",
    "}\n",
    "# add dict for the volo files\n",
    "\n",
    "# dict for the Welten-Sutter map reference files, downloaded from here: https://www.infoflora.ch/de/daten/artenliste-welten-sutter.html\n",
    "# all observations included in this report were conducted within one of these geographic boundaries\n",
    "ws_lists ={\n",
    "    \"151\":\"AtlasWS_151_Biel.csv\",\n",
    "    \"252\":\"AtlasWS_252_Erlach.csv\",\n",
    "    \"300\":\"AtlasWS_300_Aarberg.csv\",\n",
    "    \"301\":\"AtlasWS_301_Bueren.csv\",\n",
    "    \"154\":\"AtlasWS_154_Grenchen.csv\",\n",
    "    \"572\":\"AtlasWS_572_Beatenberg.csv\",\n",
    "    \"573\":\"AtlasWS_573_Interlaken.csv\",\n",
    "    \"226\":\"AtlasWS_226_Estavayer.csv\",\n",
    "    \"251\":\"AtlasWS_251_BernWest.csv\",\n",
    "    \"145\":\"AtlasWS_145_LesRangiers.csv\"\n",
    "}\n",
    "\n",
    "# housekeeping: 585220 is separated by \",\" not \";\" like the rest of the data sources\n",
    "df = pd.read_csv(\"resources/atlas5x5/Atlas5x5_585_220.csv\", sep = \",\", encoding=\"utf-16\")\n",
    "df.to_csv('resources/atlas5x5/Atlas5x5_585_220_1.csv', sep=';', encoding = \"utf-16\", index = False)\n",
    "\n",
    "# 5X5 Flora helvitca lists\n",
    "fx_lists = {\n",
    "    \"585215\":\"Atlas5x5_585_215.csv\", # Ipsach, Bielersee\n",
    "    \"585220\":\"Atlas5x5_585_220_1.csv\", # Biel Stadt, Suze / Bielersee\n",
    "    \"580220\":\"Atlas5x5_580_220.csv\", # Biel Mett, Suze\n",
    "    \"580215\":\"Atlas5x5_580_215.csv\", # Port, Nidau-Bueren Kanal\n",
    "    \"625165\":\"Atlas5x5_625_165.csv\", # Untersee, Thunersee\n",
    "    \"625170\":\"Atlas5x5_625_170.csv\", # Sundlauenen, Thunersee\n",
    "    \"550185\":\"Atlas5x5_550_185.csv\", # Estavayer, Lac de Neuchatel\n",
    "    \"575210\":\"Atlas5x5_575_210.csv\", # Leuecherz, Bielersee\n",
    "    \"600200\":\"Atlas5x5_600_200.csv\", # Bern west, Aare\n",
    "    \"575245\":\"Atlas5x5_575_245.csv\", # Saint-Ursanne, Aare\n",
    "    \"545180\":\"Atlas5x5_545_180.csv\",\n",
    "    \"575215\":\"Atlas5x5_575_215.csv\"\n",
    "}\n",
    "\n",
    "# convenience method to gather up all the files:\n",
    "def get_the_data(file_exts, a_dir, methods, this_method=\"csv\", myencoding=None):\n",
    "    wiw = {}\n",
    "    for k,v in file_exts.items():\n",
    "        if myencoding == None:\n",
    "            wiw.update({k:methods[this_method](F\"{a_dir}{v}\")})            \n",
    "        else:\n",
    "            wiw.update({k:methods[this_method](F\"{a_dir}{v}\",sep = \";\", encoding=myencoding)})\n",
    "    return wiw\n",
    "\n",
    "# use the get_the_data method to collect these files\n",
    "data_and_keys = get_the_data(d_files, data_2020, my_data_methods, this_method=\"csv\")\n",
    "watch_lists = get_the_data(w_lists, flora_h, my_data_methods, this_method=\"csv\")\n",
    "\n",
    "# Why are we usig utf-16 here? Don't we lose some options later on?\n",
    "welt_sut =  get_the_data(ws_lists, flora_h_ws, my_data_methods, this_method=\"csv\", myencoding = \"utf-16\" )\n",
    "fivex =  get_the_data(fx_lists, flora_h_55, my_data_methods, this_method=\"csv\", myencoding = \"utf-16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-multimedia",
   "metadata": {},
   "source": [
    "## The Species columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-program",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that each data set has the column \"species\", with the value species:\n",
    "watch_lists[\"list_2014\"][\"species\"] = watch_lists[\"list_2014\"].Latin\n",
    "watch_lists[\"under_sampled\"][\"species\"] = watch_lists[\"under_sampled\"].taxon\n",
    "watch_lists[\"red_list\"][\"species\"] = watch_lists[\"red_list\"].scientific_name\n",
    "watch_lists[\"cert_list\"][\"species\"] = watch_lists[\"cert_list\"][\"Short Name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-cancer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a species slug (genus-species) to link data from across the survey and reference files. \n",
    "# This is necessary as some species columns have only \"Genus species\", some include subspecies, and some include the taxonomic reference.\n",
    "\n",
    "# function to make the species slugs\n",
    "def to_species_slug(x):\n",
    "    try: \n",
    "        int_data = x.split()\n",
    "        data = int_data[:2]\n",
    "        data = \"-\".join(data)\n",
    "        data = data.lower()\n",
    "    except:\n",
    "        data = \"none\"\n",
    "    return data\n",
    "\n",
    "# create a new column to hold the slug\n",
    "for element in [fivex, welt_sut, watch_lists]:\n",
    "    for the_data in element:\n",
    "        element[the_data]['species_slug'] = 'none'\n",
    "\n",
    "# make the species slug for all reference files\n",
    "for element in [fivex, welt_sut, watch_lists]:\n",
    "    for the_data in element:\n",
    "        element[the_data]['species_slug'] = element[the_data].species.map(lambda x: to_species_slug(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceived-parade",
   "metadata": {},
   "source": [
    "## The _map_ column and the _spec\\_map_ columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-volleyball",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add identifying columns to the reference datasets\n",
    "\n",
    "# add a column to identify the map source for the geographic data:\n",
    "for element in [fivex, welt_sut]:\n",
    "    for the_data in element:\n",
    "        element[the_data]['map'] = the_data\n",
    "        element[the_data]['spec_map'] = list(zip(element[the_data].species_slug,element[the_data].map))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-median",
   "metadata": {},
   "outputs": [],
   "source": [
    "fivex.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-wesley",
   "metadata": {},
   "outputs": [],
   "source": [
    "fivex['585215'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biblical-position",
   "metadata": {},
   "source": [
    "## The _watch\\_list_ column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amino-islam",
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in [watch_lists]:\n",
    "    for the_data in element:\n",
    "        if the_data == \"list_2014\":\n",
    "            element[the_data]['watch_list'] = element[the_data][the_data]\n",
    "        else:\n",
    "            element[the_data]['watch_list'] = the_data\n",
    "\n",
    "# housekeeping: fill in nan values in the watchlist and certification list reference files.\n",
    "fill_nans = watch_lists[\"list_2014\"].copy()\n",
    "fill_nans = fill_nans.fillna(0)\n",
    "watch_lists.update({\"list_2014\":fill_nans[fill_nans.watch_list != 0]})\n",
    "\n",
    "fill_nans = watch_lists[\"cert_list\"].copy()\n",
    "fill_nans = fill_nans.fillna(0)\n",
    "watch_lists.update({\"cert_list\":fill_nans[fill_nans.watch_list != 0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-forty",
   "metadata": {},
   "outputs": [],
   "source": [
    "watch_lists.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criminal-workstation",
   "metadata": {},
   "outputs": [],
   "source": [
    "watch_lists['red_list'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-culture",
   "metadata": {},
   "source": [
    "### Species name and observations: harmonizing taxonomy\n",
    "\n",
    "The genus-species nomenclature will be used to group observations.\n",
    "\n",
    "All observations will be classified according to that standard. As a result subspecies will be folded in with the parent species. This is a reflection of the survey method and the expectation of reasonable results, not a prioritization of importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interesting-progressive",
   "metadata": {},
   "outputs": [],
   "source": [
    "def account_for_subspecies(an_array, a_dict):\n",
    "    for element in an_array:\n",
    "        try:\n",
    "            a_dict[element[0]].append(element[1])\n",
    "        except:\n",
    "            a_dict[element[0]] = [element[1]]\n",
    "    return a_dict\n",
    "a_dict ={}\n",
    "\n",
    "for element in [fivex, welt_sut, watch_lists]:\n",
    "    for label in element:\n",
    "        # use this data frame\n",
    "        som_data = element[label].copy()\n",
    "        \n",
    "        # group by species slug and count the number of unique species values\n",
    "        c_s_p_s = som_data.groupby('species_slug', as_index=False).species.nunique()        \n",
    "        \n",
    "        # just the records with more than one species value\n",
    "        m_t_one = c_s_p_s[c_s_p_s.species > 1].species_slug\n",
    "        \n",
    "        # pair the species_slug to the species name:\n",
    "        mto = som_data.loc[som_data.species_slug.isin(m_t_one)][['species_slug', 'species']].copy().to_numpy()\n",
    "        \n",
    "        # update the dict\n",
    "        account_for_subspecies(mto, a_dict)\n",
    "\n",
    "# the species_slugs that account for more than one sub species\n",
    "sub_species_accounted =  {k:list(set(v)) for k,v in a_dict.items()}\n",
    "\n",
    "# the species_slugs\n",
    "gs_parent = list(sub_species_accounted.keys())\n",
    "\n",
    "# the number of species_slugs\n",
    "number_of_gs = len(gs_parent)\n",
    "\n",
    "# the the number of sub species accounted for\n",
    "number_of_ss = sum([len(v) for k,v in sub_species_accounted.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-locator",
   "metadata": {},
   "source": [
    "## Determine wether or not a species was detected within a geographic limit\n",
    "\n",
    "The territory is divided into different segments. Flora-helvitica and WS maps have different geographic bounds. Here the presence or not of a species within the confines of one of the different boundaries is determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-petersburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all the observations from flora helvitaca and the WS into one df\n",
    "fx = pd.concat([v[['species_slug', 'map', 'spec_map']] for k,v in fivex.items()])\n",
    "wsx = pd.concat([v[['species_slug', 'map', 'spec_map']] for k,v in welt_sut.items()])\n",
    "f_w_obs = pd.concat([fx, wsx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36940cac-8e2c-4959-929b-c75968b313ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_w_obs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e820fa-20aa-41b5-b20b-d8904d797ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacedict = {\n",
    "#     'verbanum bonariensis ':'verbena bonariensis',\n",
    "#     'medicago varia':'medicago sativa',\n",
    "#     \"oenothera\":\"oenothera biennis\",\n",
    "#     \"geranium pratens\":\"geranium pratense\",\n",
    "#     \"oenothera biennis \": \"oenothera biennis\",\n",
    "#     \"oenothera biennis agg.\": \"oenothera biennis\",\n",
    "#     \"solidalgo canadensis\": \"solidago canadensis\",\n",
    "#     \"verbascum lynchitis\":\"verbascum lychnitis\",\n",
    "#     \"verbascum negris\":\"verbascum nigrum\",\n",
    "#     \"securigea varia\": \"securigera varia\",\n",
    "#     \"melilotus officianalis\": \"melilotus officinalis\",\n",
    "#     \"knautia maxima\": \"knautia dipsacifolia\",\n",
    "#     \"hieracium aurantiacum\":\"pilosella aurantiaca\",\n",
    "#     \"sysimbrium officinale\":\"sisymbrium officinale\",\n",
    "#     \"geranium robertanium\":\"geranium robertianum\",\n",
    "#     \"mycelis muralis\": \"lactuca muralis\",\n",
    "#     \"calamintha-nepeta\":\"clinopodium nepeta\",\n",
    "#     \"polygonum-persicaria\":\"persicaria maculosa\",\n",
    "#     \"sorbus-aria\":\"aria edulis\",\n",
    "#     \"taraxacum\": \"taraxacum officinale\",\n",
    "#     \"jacobaea vulgaris\" : \"senecio jacobaea\",\n",
    "#     \"erigeron canadensis\" : \"conyza canadensis\",\n",
    "#     \"rorippa islandica\" : \"rorippa palustris\",\n",
    "#     \"malus sylvestris\" : \"malus domestica\",\n",
    "#     \"hylotelephium telephium\" : \"sedum telephium\",\n",
    "#     \"lactuca muralis\": \"mycelis muralis\",\n",
    "#     \"chaenorhinum minus\": \"chaenorrhinum minus\",\n",
    "#     \"erigeron canadensis\": \"conzya canadensis\",\n",
    "#     \"erigeron canadensis\": \"conzya canadensis\",\n",
    "#     \"borkhausenia intermedia\": \"scandosorbus intermedia\",\n",
    "#     \"centaurea nigra\" : \"centaurea jacea\"\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2ef6b1-a02c-43b7-9248-f7eb016d4418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_w_obs['ns'] = f_w_obs.species_slug\n",
    "# def replace_this(x,a_dict):\n",
    "#     if x in a_dict.keys():\n",
    "#         data=a_dict[x]\n",
    "#     else:\n",
    "#         data = x\n",
    "#     return data\n",
    "# f_w_obs['ns'] = f_w_obs.ns.map(lambda x: replace_this(x, replacedict))\n",
    "# f_w_obs['species_slug'] = f_w_obs.ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-jesus",
   "metadata": {},
   "outputs": [],
   "source": [
    "astring = F\"\"\"\n",
    "There are {len(f_w_obs.map.unique())} different map boundaries in this study\n",
    "\"\"\"\n",
    "md(astring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e810772-748d-4815-8655-c79a53aea479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = f_w_obs.set_index('species_slug')\n",
    "# a.loc['bryonia-dioica']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-welcome",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather up the map names\n",
    "m_ap_columns = f_w_obs.map.unique()\n",
    "\n",
    "# create a column for each map, indicate\n",
    "for col in m_ap_columns:\n",
    "    \n",
    "    f_w_obs[col] = f_w_obs['map'] == col\n",
    "\n",
    "obs_map =f_w_obs.groupby(['species_slug']).sum()\n",
    "\n",
    "# human readable column names need to be introduced here or a dict to rename\n",
    "obs_map.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f20d736-f344-4796-ba70-ae8add44c469",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_map.loc['bryonia-dioica']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-induction",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_file_name = \"species_map_located.csv\"\n",
    "obs_map.to_csv(F\"resources/survey-data/{a_file_name}\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superb-resistance",
   "metadata": {},
   "source": [
    "### Key the species to the different maps it was identified in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-marks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #exports the dictionary to a .json file\n",
    "# nt = obs_map['maps'].to_dict()\n",
    "\n",
    "# ntx = {k:list(v) for k,v in nt.items()}\n",
    "\n",
    "# with open(F\"{here}/output/ws_list.json\",\"w\") as afile:\n",
    "#     json.dump(ntx,afile)\n",
    "    \n",
    "# print(F\"\\nWhich maps 'trifolium-incarnatum' were found in? indifferent of subspecies?:\\n\\n{ntx['trifolium-incarnatum']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-economics",
   "metadata": {},
   "source": [
    "## The _species\\_slug_ column\n",
    "\n",
    "A coder friendly way to find the species but still maintain the proper nomenclature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powerful-picnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format scientific name in the sample dataset\n",
    "samples = data_and_keys['surveys'].copy()\n",
    "\n",
    "samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cb7790-181e-4d9b-a3a8-64b5e60f8f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of replacement values that are incorrect\n",
    "replacedict = {\n",
    "    'verbanum bonariensis ':'verbena bonariensis',\n",
    "    'medicago varia':'medicago sativa',\n",
    "    \"oenothera\":\"oenothera biennis\",\n",
    "    \"geranium pratens\":\"geranium pratense\",\n",
    "    \"oenothera biennis \": \"oenothera biennis\",\n",
    "    \"oenothera biennis agg.\": \"oenothera biennis\",\n",
    "    \"solidalgo canadensis\": \"solidago canadensis\",\n",
    "    \"verbascum lynchitis\":\"verbascum lychnitis\",\n",
    "    \"verbascum negris\":\"verbascum nigrum\",\n",
    "    \"securigea varia\": \"securigera varia\",\n",
    "    \"melilotus officianalis\": \"melilotus officinalis\",\n",
    "    \"knautia maxima\": \"knautia dipsacifolia\",\n",
    "    \"hieracium aurantiacum\":\"pilosella aurantiaca\",\n",
    "    \"sysimbrium officinale\":\"sisymbrium officinale\",\n",
    "    \"geranium robertanium\":\"geranium robertianum\",\n",
    "    \"mycelis muralis\": \"lactuca muralis\",\n",
    "    \"calamintha-nepeta\":\"clinopodium nepeta\",\n",
    "    \"polygonum-persicaria\":\"persicaria maculosa\",\n",
    "    \"sorbus-aria\":\"aria edulis\",\n",
    "    \"taraxacum\": \"taraxacum officinale\",\n",
    "    \"jacobaea vulgaris\" : \"senecio jacobaea\",\n",
    "    \"erigeron canadensis\" : \"conyza canadensis\",\n",
    "    \"rorippa islandica\" : \"rorippa palustris\",\n",
    "    \"malus sylvestris\" : \"malus domestica\",\n",
    "    \"hylotelephium telephium\" : \"sedum telephium\",\n",
    "    \"lactuca muralis\": \"mycelis muralis\",\n",
    "    \"chaenorhinum minus\": \"chaenorrhinum minus\",\n",
    "    \"erigeron canadensis\": \"conzya canadensis\",\n",
    "    \"erigeron canadensis\": \"conzya canadensis\",\n",
    "    \"borkhausenia intermedia\": \"scandosorbus intermedia\",\n",
    "    \"centaurea nigra\" : \"centaurea jacea\"\n",
    "}\n",
    "\n",
    "# function to assign the correct value of the key is in the samples dictionary.\n",
    "def new_func(x,keys):\n",
    "    try:\n",
    "        data = keys[x]\n",
    "    except:\n",
    "        data = x\n",
    "    return data\n",
    "\n",
    "# apply the funtion to a copy of the surveys data set.\n",
    "samples[\"species2\"] = samples.sci.map(lambda x: new_func(x, replacedict))\n",
    "samples[\"species_slug\"] = samples.species2.map(lambda x: to_species_slug(x))\n",
    "\n",
    "# update the surveys dataset.\n",
    "data_and_keys.update({'surveys':samples})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-developer",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_and_keys[\"surveys\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-percentage",
   "metadata": {},
   "source": [
    "## Format date column to ISO standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-equation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format dates in the sample dataset\n",
    "\n",
    "# function converts DD.MM.YYYY format to YYYY-MM-DD format, ignores if already in YYYY-MM-DD format\n",
    "# Do we even need this any more ? Looking at the cell above it appears the dates have been fixed at the source\n",
    "def change_string(x):\n",
    "    try:\n",
    "        s_data = x.split('.')\n",
    "        data = s_data[::-1]\n",
    "        data = \"-\".join(data)\n",
    "    except:\n",
    "        print(\"no luck\")\n",
    "        data = x\n",
    "    \n",
    "    return data\n",
    "\n",
    "# applies the function to a column in the samples data frame\n",
    "samples['new_date'] = samples.date.map(lambda x: change_string(x))\n",
    "\n",
    "# function makes a timestamp out of the YYYY-MM-DD string.\n",
    "# def make_timestamp(x):\n",
    "#     try:        \n",
    "#         data = dt.datetime.strptime(x, \"%Y-%m-%d\")        \n",
    "#     except:        \n",
    "#         data = 'no luck'\n",
    "    \n",
    "#     return data\n",
    "\n",
    "# run the make_timestamp function and store the results in the samples dataframe.\n",
    "# samples['stamp_date'] = samples.new_date.map(lambda x: make_timestamp(x))\n",
    "# samples['date'] = samples.stamp_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-receiver",
   "metadata": {},
   "source": [
    "## The _place1_ column\n",
    "\n",
    "A coder friendly way to find the species but still maintain the proper nomenclature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-toddler",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make place name slugs in the sample dataset\n",
    "#  change_place function turns \"place names 1\" into place-names-1\n",
    "def change_place(x):\n",
    "    data = x.split(\" \")\n",
    "    data = \"-\".join(data)\n",
    "    return data\n",
    "\n",
    "samples[\"place_slug\"] = samples.place.map(lambda x: change_place(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "duplicate-device",
   "metadata": {},
   "source": [
    "## Housekeeping and export of processed survey data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81901b7-1fdf-4f5e-a074-acf6b491f8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up redundant and unused column names\n",
    "# rename the place_slug column\n",
    "\n",
    "samples.rename(columns= {'place1':'place_slug', 'new_date':'str_date'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06d9454-672f-43a0-8561-11cc9bb0a38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instructional-stupid",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the unnecessary columns for this analysis\n",
    "samples.drop(['time',  'sci', 'name', 'place', 'species2'] , inplace=True, axis=1)\n",
    "\n",
    "samples['loc_date'] = list(zip(samples.place_slug, samples.date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ideal-antibody",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export samples to a .csv for later use\n",
    "samples.to_csv(F\"resources/preprocessed/hd_samples_2020.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intensive-logging",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-married",
   "metadata": {},
   "source": [
    "Subspecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-intro",
   "metadata": {},
   "outputs": [],
   "source": [
    "def account_for_subspecies(an_array, a_dict):\n",
    "    for element in an_array:\n",
    "        try:\n",
    "            a_dict[element[0]].append(element[1])\n",
    "        except:\n",
    "            a_dict[element[0]] = [element[1]]\n",
    "    return a_dict\n",
    "a_dict ={}\n",
    "\n",
    "for element in [fivex, welt_sut, watch_lists]:\n",
    "    for label in element:\n",
    "        # use this data frame\n",
    "        som_data = element[label].copy()\n",
    "        \n",
    "        # group by species slug and count the number of unique species values\n",
    "        c_s_p_s = som_data.groupby('species_slug', as_index=False).species.nunique()        \n",
    "        \n",
    "        # just the records with more than one species value\n",
    "        m_t_one = c_s_p_s[c_s_p_s.species > 1].species_slug\n",
    "        \n",
    "        # pair the species_slug to the species name:\n",
    "        mto = som_data.loc[som_data.species_slug.isin(m_t_one)][['species_slug', 'species']].copy().to_numpy()\n",
    "        \n",
    "        # update the dict\n",
    "        account_for_subspecies(mto, a_dict)\n",
    "\n",
    "# the species_slugs that account for more than one sub species\n",
    "sub_species_accounted =  {k:list(set(v)) for k,v in a_dict.items()}\n",
    "\n",
    "# the species_slugs\n",
    "gs_parent = list(sub_species_accounted.keys())\n",
    "\n",
    "# the number of species_slugs\n",
    "number_of_gs = len(gs_parent)\n",
    "\n",
    "# the the number of sub species accounted for\n",
    "number_of_ss = sum([len(v) for k,v in sub_species_accounted.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-involvement",
   "metadata": {},
   "source": [
    "### Key the watch lists to genus-species\n",
    "\n",
    "Identify which species are on which watch list. Produce a boolean matrix with species_slug as index and watch list name for the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-meter",
   "metadata": {},
   "outputs": [],
   "source": [
    "the_lists = list(watch_lists.keys())\n",
    "\n",
    "keep = []\n",
    "\n",
    "for element in the_lists:\n",
    "    a = watch_lists[element][['species_slug','watch_list']]\n",
    "    keep.append(a)\n",
    "wl_species = pd.concat(keep)\n",
    "m_ap_columns = wl_species.watch_list.unique()\n",
    "\n",
    "for col in m_ap_columns:    \n",
    "    wl_species[col] = wl_species['watch_list'] == col\n",
    "\n",
    "wl_sp_map =wl_species.groupby(['species_slug']).sum()\n",
    "\n",
    "wl_sp_map['lists'] = wl_sp_map.index.map(lambda x: wl_species[wl_species.species_slug == x]['watch_list'].unique())\n",
    "\n",
    "a_filename = \"species_keyed_to_watch_list.csv\"\n",
    "wl_sp_map.reset_index().to_csv(F\"resources/{a_filename}\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greenhouse-moisture",
   "metadata": {},
   "outputs": [],
   "source": [
    "wl_sp_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-sweet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nt = wl_sp_map['lists'].to_dict()\n",
    "\n",
    "# ntx = {k:list(v) for k,v in nt.items()}\n",
    "\n",
    "# with open(F\"{here}/output/flora_list.json\",\"w\") as afile:\n",
    "#     json.dump(ntx,afile)\n",
    "# wl_sp_map.to_csv(F\"{here}/resources/preprocessed/aggregated_list.csv\",index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5a1226-19dd-4d69-8249-eb821d83ca3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
