{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "common-spanish",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "stunning-sperm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# math and data packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# charting and graphics\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# os and file types\n",
    "import os\n",
    "import sys\n",
    "import datetime as dt\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# images and display\n",
    "import base64, io, IPython\n",
    "from PIL import Image as PILImage\n",
    "from IPython.display import Markdown as md\n",
    "from IPython.display import display, Math, Latex\n",
    "\n",
    "\n",
    "# set useful variables for accessing the files\n",
    "here = os.getcwd()\n",
    "resources = F\"{here}/resources/\"\n",
    "flora_h_ws = F\"{resources}atlasws/\"\n",
    "flora_h_55 = F\"{resources}atlas5x5/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "drawn-individual",
   "metadata": {},
   "source": [
    "## Import data\n",
    "\n",
    "Fix any known formatting problems here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94ddcdff-733e-4a53-a662-602c6642e5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 935 entries, 0 to 934\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   place   935 non-null    object\n",
      " 1   date    935 non-null    object\n",
      " 2   time    756 non-null    object\n",
      " 3   sci     935 non-null    object\n",
      " 4   name    671 non-null    object\n",
      "dtypes: object(5)\n",
      "memory usage: 36.6+ KB\n"
     ]
    }
   ],
   "source": [
    "survey_data = pd.read_csv(\"resources/surveys.csv\")\n",
    "survey_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "052ec47f-4b0c-434c-8e11-33d77d8b87ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>place</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>sci</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alleestrasse 1</td>\n",
       "      <td>2020-09-02</td>\n",
       "      <td>11:09:04 AM</td>\n",
       "      <td>plantago lanceolata</td>\n",
       "      <td>plantain lancéolé</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alleestrasse 1</td>\n",
       "      <td>2020-09-02</td>\n",
       "      <td>11:08:19 AM</td>\n",
       "      <td>centaurea nigra</td>\n",
       "      <td>centaurée noire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alleestrasse 1</td>\n",
       "      <td>2020-09-02</td>\n",
       "      <td>11:06:59 AM</td>\n",
       "      <td>plantago media</td>\n",
       "      <td>plantain moyen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alleestrasse 1</td>\n",
       "      <td>2020-09-02</td>\n",
       "      <td>11:06:05 AM</td>\n",
       "      <td>chenopodium album agg.</td>\n",
       "      <td>chénopode blanc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alleestrasse 1</td>\n",
       "      <td>2020-09-02</td>\n",
       "      <td>11:05:18 AM</td>\n",
       "      <td>centaurea jacea agg.</td>\n",
       "      <td>centaurée jacée</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            place        date         time                     sci  \\\n",
       "0  alleestrasse 1  2020-09-02  11:09:04 AM     plantago lanceolata   \n",
       "1  alleestrasse 1  2020-09-02  11:08:19 AM         centaurea nigra   \n",
       "2  alleestrasse 1  2020-09-02  11:06:59 AM          plantago media   \n",
       "3  alleestrasse 1  2020-09-02  11:06:05 AM  chenopodium album agg.   \n",
       "4  alleestrasse 1  2020-09-02  11:05:18 AM    centaurea jacea agg.   \n",
       "\n",
       "                name  \n",
       "0  plantain lancéolé  \n",
       "1    centaurée noire  \n",
       "2     plantain moyen  \n",
       "3    chénopode blanc  \n",
       "4    centaurée jacée  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "survey_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1f2d8c3-87ae-4a58-ab27-e444911ede79",
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_data['loc_date'] = list(zip(survey_data.place, survey_data['date']))\n",
    "places = survey_data.place.unique()\n",
    "nsurveys = survey_data.loc_date.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3f7e1b5-eb60-4955-8010-add2f331f1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "scis = survey_data.sci.unique()\n",
    "names = survey_data.name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce999403-2b6b-4933-be2f-48d26678c01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111 110\n"
     ]
    }
   ],
   "source": [
    "# match place names to map coordinates\n",
    "map_keys = pd.read_csv(\"resources/map-keys.csv\")\n",
    "mplaces = map_keys.place.unique()\n",
    "print(len(places), len(mplaces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4dbecafa-5294-4633-972c-83da816637d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alleestrasse 1', 'alleestrasse 2', 'alleestrasse 3', 'alleestrasse 4', 'bluets 1', 'bluets 2', 'bluets 3', 'boujean 1', 'boujean 2', 'boujean 3', 'buren 1', 'buren 2', 'buren 3', 'cff lot 1', 'cff lot 2', 'cff path 1', 'cff path 2', 'cff path 3', 'chemin de la course', 'chemin des voies', 'cheyres path', 'cheyres reserve', 'cygnes lot 1', 'cygnes lot 2', 'cygnes lot 3', 'cygnes lot 4', 'cygnes lot 5', 'cygnes lot 6', 'cygnes lot 7', 'cygnes lot 8', 'energie 1', 'energie 2', 'energie 3', 'football 1', 'football 2', 'football 3', 'football 4', 'football 5', 'football 6', 'frinvillier fabrique 1', 'frinvillier fabrique 2', 'frinvillier fabrique 3', 'frinvillier fabrique 4', 'frinvillier ramp', 'gottstatt 1', 'gottstatt 2', 'gottstatt 3', 'hayek 1', 'jura 1', 'jura 2', 'lezard 1', 'ligerz favorite', 'lucherz bureli', 'lucherz seestrasse', 'orvin petit moulin', 'pery taubenlochweg', 'pieterlen stockweg', 'rue alfred-aebi', 'rue centrale', 'rue de leau 1', 'rue de leau 2', 'rue de leau 3', 'rue de leau 4', 'rue de leau 5', 'saint ursanne falls', 'salome 1', 'salome 2', 'salome 3', 'salome 4', 'salome 5', 'schlosslifeld 1', 'schlosslifeld 2', 'schlosslifeld 3', 'schussinsel 3', 'schussinsel 1', 'schussinsel 2', 'schussinsel 4', 'schussinsel 5', 'schussinsel 6', 'sonnenfeld 1', 'sonnenfeld 2', 'sundgraben beach', 'sundgraben bridge left', 'sundgraben forest left', 'sundgraben forest right', 'sundgraben gorge', 'sundgraben port', 'lezard 2', 'parc muni', 'swatch 1', 'swatch 2', 'taubenloch 1', 'taubenloch 2', 'transjurane tunnel', 'truite 1', 'truite 2', 'truite 3', 'truite 4', 'truite 5', 'twann estuary', 'twann gorges']\n",
      "['jura-2', 'sundgraben-port', 'salome-3', 'salome-4', 'alleestrasse-4', 'energie-1', 'boujean-1', 'boujean-2', 'bluets-2', 'energie-3', 'boujean-3', 'alleestrasse-3', 'alleestrasse-2', 'alleestrasse-1', 'sonnenfeld-1', 'salome-5', 'bluets-3', 'football-1', 'football-3', 'football-2', 'football-4', 'football-5', 'schlosslifeld-2', 'schlosslifeld-3', 'schlosslifeld-1', 'sonnenfeld-2', 'cygnes-lot-1', 'frinvillier-fabrique-2', 'frinvillier-fabrique-1', 'frinvillier-fabrique-3', 'cygnes-lot-2', 'frinvillier-fabrique-4', 'cygnes-lot-3', 'cygnes-lot-4', 'schussinsel-1', 'schussinsel-2', 'cygnes-lot-5', 'lezard-1', 'cff-lot-1', 'cff-lot-2', 'transjurane-tunnel', 'pery-taubenlochweg', 'orvin-petit-moulin', 'cheyres-path', 'cheyres-reserve', 'sundgraben-gorge', 'sundgraben-forest-left', 'sundgraben-forest-right', 'cff-path-1', 'schussinsel-3', 'cygnes-lot-6', 'lucherz-seestrasse', 'lucherz-bureli', 'ligerz-favorite', 'cygnes-lot-7', 'cygnes-lot-8', 'twann-estuary', 'twann-gorges', 'salome-1', 'jura-1', 'cff-path-2', 'cff-path-3', 'sundgraben-bridge-left', 'buren-1', 'buren-2', 'buren-3', 'schussinsel-4', 'schussinsel-5', 'schussinsel-6', 'hayek-1', 'swatch-2', 'swatch-1', 'chemin-de-la-course', 'bluets-1', 'truite-1', 'truite-2', 'truite-3', 'truite-4', 'truite-5', 'rue-de-leau-1', 'rue-de-leau-2', 'rue-de-leau-3', 'rue-de-leau-4', 'rue-de-leau-5', 'energie-2', 'salome-2', 'chemin-des-voies', 'frinvillier-ramp', 'pieterlen-stockweg', 'rue-alfred-aebi', 'rue-centrale', 'saint-ursanne-falls', 'sundgraben-beach', 'gottstatt-2', 'gottstatt-1', 'gottstatt-3', 'football-6', 'lezard-2', 'parc-muni', 'taubenloch-1']\n"
     ]
    }
   ],
   "source": [
    "# find the difference\n",
    "\n",
    "in_survey_data = [x for x in places if x not in mplaces]\n",
    "in_map_data = [ x for x in mplaces if x not in places]\n",
    "\n",
    "# these need to be slugged\n",
    "print(in_survey_data)\n",
    "print(in_map_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2913ee1f-3011-4e89-844c-a22b2422ad82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Latin', 'French', 'Jura', 'Plateau', 'Versant Nord des Alpes',\n",
       "       'Alpes centrales Ouest', 'Alpes centrales Est', 'Versant Sud des Alpes',\n",
       "       'non établi en Suisse', '1 Eau1 libres', '2 Rivages et lieu humides',\n",
       "       '3 Glaciers, rochers, éboulis et moraines', '4 Pelouses et prairies',\n",
       "       '5 Landes, lisières et mégaphorbiaies', '6 Forêts',\n",
       "       '7 Végétations pionnières des endroits perturbés',\n",
       "       '8 Plantations, champs, cultures', '9 Milieu1 construits',\n",
       "       'Potentiel d'e1pansion', 'santé', 'écologie, biodiversité', 'économie',\n",
       "       'Präventionscharakter (hoch=1, mittel=2, gering=3)',\n",
       "       'Regionale Wichtigkeit (hoch=1, mittel=2, gering=3)',\n",
       "       'Lack List / Watch List \"old\"', 'list_2014',\n",
       "       'Ordonnonce sur la dissémination des organismes (ODE)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plant names\n",
    "invsvs = pd.read_csv(\"resources/inprocess/invasives.csv\")\n",
    "invsvs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b6d1d39-fffe-4907-8258-50a7f73ac7c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            Abutilon theophrasti Medik.\n",
       "1    Ailanthus altissima (Mill.) Swingle\n",
       "2             Ambrosia artemisiifolia L.\n",
       "3                   Amorpha fruticosa L.\n",
       "4          Artemisia verlotiorum Lamotte\n",
       "Name: Latin, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invsvs.Latin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd7bec13-d897-4b83-8fdb-75186a8fdb12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([' ID taxon', 'species', 'nom allemand', 'nom francais', 'nom italien',\n",
       "       'priorite', 'menace', 'responsabilite',\n",
       "       'necessite de prendre des mesures',\n",
       "       'Necessite de surveiller les populations', 'connaissances suffisantes?',\n",
       "       'techniques connues?', 'Jura', 'Plateau'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "priority = pd.read_csv(\"resources/inprocess/priority.csv\")\n",
    "priority.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a691d1d-e6f0-4561-88ef-c0137848ce1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                   Achillea atrata L.\n",
       "1                                 Achillea clavenae L.\n",
       "2                               Achillea collina Rchb.\n",
       "3    Achillea erba-rotta subsp. moschata (Wulfen) V...\n",
       "4                              Achillea macrophylla L.\n",
       "Name: species, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "priority.species.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7736692e-3b56-4373-aabc-72fa6733e3fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['FAMILY', 'GENUS', 'species', 'Deutscher Name', 'Nom en francais', 'CH',\n",
       "       'crit_CH', 'JU', 'crit_JU', 'MP', 'crit_MP'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redlist = pd.read_csv(\"resources/inprocess/redlist.csv\")\n",
    "redlist.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ae6aace-9be1-4934-b094-cce4790d82bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             Abies alba\n",
       "1         Acer campestre\n",
       "2            Acer opalus\n",
       "3       Acer platanoides\n",
       "4    Acer pseudoplatanus\n",
       "Name: species, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redlist.species.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e42ba489-9263-46fe-b3c3-db4848cc8b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start by organizing them into dictionaries.\n",
    "my_data_methods = {\"csv\":pd.read_csv}\n",
    "\n",
    "# import the survey data files\n",
    "survey_files = {\n",
    "    \"surveys_20\":\"surveys.csv\",\n",
    "    \"surveys_21a\":\"data-2021.csv\",\n",
    "    \"surveys_21b\":\"surveys-21-vf.csv\",\n",
    "    \"map_keys_20\":\"map-keys.csv\",\n",
    "    \"map_keys_21\":\"2021-survey-key.csv\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a023e4e5-24f1-4427-b07f-b17782dc327a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'resources/data-2021.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16783/2109187036.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0msurveys_20\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"resources/surveys.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0msurveys_21a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"resources/data-2021.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0msurveys_21b\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"resources/surveys-21-vf.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-16\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mkeys_20\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"resources/map-keys.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ppenv/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ppenv/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ppenv/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ppenv/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ppenv/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ppenv/lib/python3.7/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ppenv/lib/python3.7/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ppenv/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'resources/data-2021.csv'"
     ]
    }
   ],
   "source": [
    "# Import the survey data files into panda data frames.\n",
    "\n",
    "# surveys_20 = 2020 survey data\n",
    "# surveys_21a = Feb - May 2021 survey data\n",
    "# surveys_21b = Jun - Oct 2021 survey data\n",
    "# keys_20 = locations keys for 2020 surveys\n",
    "# keys_21a = location keys for Feb-May 2021 surveys\n",
    "\n",
    "surveys_20 = pd.read_csv(\"resources/surveys.csv\")\n",
    "surveys_21a = pd.read_csv(\"resources/data-2021.csv\")\n",
    "surveys_21b= pd.read_csv(\"resources/surveys-21-vf.csv\", encoding = \"utf-16\")\n",
    "keys_20 = pd.read_csv(\"resources/map-keys.csv\")\n",
    "keys_21a = pd.read_csv(\"resources/2021-survey-key.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c435d3-8313-450c-b368-965d5149748f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict for the reference files that are not Welten Sutter or Flora Helvetica 5X5 lists\n",
    "\n",
    "# watch_list = list of invasive and potentially invasive species\n",
    "# red_list = conservation status of indigenous species\n",
    "# priority_list = priority species for conservation in Switzerland\n",
    "\n",
    "off_lists = {\n",
    "    \"list_2014\":\"BL_WL_2014_modified.csv\",\n",
    "    \"under_sampled\":\"taxa_sous_echantillonnes.csv\",\n",
    "    \"red_list\":\"redlist2019.csv\",\n",
    "    \"cert_list\": \"Certification_specieslist_2021.csv\",\n",
    "    \"ch_priority\":\"ch_priority_species.csv\"\n",
    "}\n",
    "\n",
    "watch_list = pd.read_csv(\"resources/BL_WL_2014_modified.csv\")\n",
    "red_list = pd.read_csv(\"resources/redlist2019.csv\")\n",
    "under_sampled = pd.read_csv(\"resources/taxa_sous_echantillonnes.csv\")\n",
    "priority_list = pd.read_csv(\"resources/ch_priority_species.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6ead80-bcb4-4ed2-ae91-44b64bf3a1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lists from Equipe Volo\n",
    "\n",
    "volo_lists = {\n",
    "    \"imp_seeds\":\"important-seeds-2021.csv\",\n",
    "    \"off_list\":\"all-seeds-2021.csv\",\n",
    "    \"inventory\":\"volo-inventory-de.csv\",\n",
    "    \"germ_exp\":\"germination-experiment.csv\"\n",
    "}\n",
    "\n",
    "# germ_exp = data from 2021 seed germination experiment\n",
    "# imp_seeds = seeds that should have been collected in 2021\n",
    "# all_seeds = all plant species in the volo catalogues for 2021\n",
    "# inventory = equipe volo seed inventory\n",
    "\n",
    "imp_seeds = pd.read_csv(\"resources/important-seeds-2021.csv\")\n",
    "off_list = pd.read_csv(\"resources/all-seeds-2021.csv\")\n",
    "inventory = pd.read_csv(\"resources/volo-inventory-de.csv\")\n",
    "germ_exp = pd.read_csv(\"resources/germination-experiment.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525f8944-e903-456e-b46f-556731bd0062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict for the Welten-Sutter map reference files, downloaded from here: www.infoflora.ch\n",
    "# all observations included in this report were conducted within one of these geographic boundaries (true for 2020 surveys)\n",
    "# need to include some 2021 surveys.\n",
    "ws_lists ={\n",
    "    \"151\":\"AtlasWS_151_Biel.csv\",\n",
    "    \"252\":\"AtlasWS_252_Erlach.csv\",\n",
    "    \"300\":\"AtlasWS_300_Aarberg.csv\",\n",
    "    \"301\":\"AtlasWS_301_Bueren.csv\",\n",
    "    \"154\":\"AtlasWS_154_Grenchen.csv\",\n",
    "    \"572\":\"AtlasWS_572_Beatenberg.csv\",\n",
    "    \"573\":\"AtlasWS_573_Interlaken.csv\",\n",
    "    \"226\":\"AtlasWS_226_Estavayer.csv\",\n",
    "    \"251\":\"AtlasWS_251_BernWest.csv\",\n",
    "    \"145\":\"AtlasWS_145_LesRangiers.csv\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbd763a-262e-48ed-b234-96c5af151949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict for the 5X5 Flora Helvetica lists downloaded from www.infoflora.ch\n",
    "\n",
    "# housekeeping: 585220 is separated by \",\" not \";\" like the rest of the data sources\n",
    "df = pd.read_csv(\"resources/atlas5x5/Atlas5x5_585_220.csv\", sep = \",\", encoding=\"utf-16\")\n",
    "df.to_csv('resources/atlas5x5/Atlas5x5_585_220_1.csv', sep=';', encoding = \"utf-16\", index = False)\n",
    "\n",
    "fx_lists = {\n",
    "    \"585215\":\"Atlas5x5_585_215.csv\", # Ipsach, Bielersee\n",
    "    \"585220\":\"Atlas5x5_585_220_1.csv\", # Biel Stadt, Suze / Bielersee\n",
    "    \"580220\":\"Atlas5x5_580_220.csv\", # Biel Mett, Suze\n",
    "    \"580215\":\"Atlas5x5_580_215.csv\", # Port, Nidau-Bueren Kanal\n",
    "    \"625165\":\"Atlas5x5_625_165.csv\", # Untersee, Thunersee\n",
    "    \"625170\":\"Atlas5x5_625_170.csv\", # Sundlauenen, Thunersee\n",
    "    \"550185\":\"Atlas5x5_550_185.csv\", # Estavayer, Lac de Neuchatel\n",
    "    \"575210\":\"Atlas5x5_575_210.csv\", # Leuecherz, Bielersee\n",
    "    \"600200\":\"Atlas5x5_600_200.csv\", # Bern west, Aare\n",
    "    \"575245\":\"Atlas5x5_575_245.csv\", # Saint-Ursanne, Aare\n",
    "    \"545180\":\"Atlas5x5_545_180.csv\",\n",
    "    \"575215\":\"Atlas5x5_575_215.csv\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1049be55-8d2a-4e3a-a945-6c26e4da7338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convenience method to gather up all the files:\n",
    "def get_the_data(file_exts, methods, this_method=\"csv\", myencoding=None):\n",
    "    wiw = {}\n",
    "    for k,v in file_exts.items():\n",
    "        if myencoding == None:\n",
    "            wiw.update({k:methods[this_method](F\"resources/{v}\")})            \n",
    "        else:\n",
    "            wiw.update({k:methods[this_method](F\"resources/{v}\",sep = \";\", encoding=myencoding)})\n",
    "    return wiw\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b13fbf-aa89-4459-9e06-c53b0ba86eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the get_the_data method to collect these files\n",
    "data_and_keys = get_the_data(survey_files, my_data_methods, this_method=\"csv\")\n",
    "watch_lists = get_the_data(w_lists, my_data_methods, this_method=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-banks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the data files\n",
    "# create variables for convenience method and grouping\n",
    "\n",
    "# start by organizing them into dictionaries.\n",
    "my_data_methods = {\"csv\":pd.read_csv}\n",
    "\n",
    "# dict for the 2020 survey data files\n",
    "d_files = {\n",
    "    \"surveys_20\":\"surveys.csv\",\n",
    "    \"surveys_21a\":\"data-2021.csv\",\n",
    "    \"surveys_21b\":\"obs_export_2021-10-26_20h46.csv\",\n",
    "    \"map_keys_20\":\"map-keys.csv\",\n",
    "    \"map_keys_21\":\"2021-survey-key.csv\"\n",
    "}\n",
    "# match map_keys_21 to surveys_21A\n",
    "# date and time format correction\n",
    "# species slug\n",
    "# format to 2020\n",
    "\n",
    "# dict for the reference files that are not Welten Sutter or Flora Helvetica 5X5 lists\n",
    "w_lists = {\n",
    "    \"list_2014\":\"BL_WL_2014_modified.csv\",\n",
    "    \"under_sampled\":\"taxa_sous_echantillonnes.csv\",\n",
    "    \"red_list\":\"redlist2019.csv\",\n",
    "    \"cert_list\": \"Certification_specieslist_2021.csv\",\n",
    "    \"ch_priority\":\"ch_priority_species.csv\"\n",
    "}\n",
    "\n",
    "# format and import ch_priority_species.csv to use with 2020 & 2021 survey results\n",
    "# evaluate the survey results with respect to the columns \"priorité\tmenace\tresponsabilité\tnécessité de prendre des mesures\tnécessité de surveiller les populations\tconnaissances suffisantes?\ttechniques connues?\n",
    "# repeat 2020 data with redlist with 2021A\n",
    "\n",
    "v_lists = {\n",
    "    \"imp_seeds\":\"important-seeds-2021.csv\",\n",
    "    \"all_seeds\":\"all-seeds-2021.csv\",\n",
    "    \"inv_cave\":\"volo-inventory-de.csv\",\n",
    "    \"germ_exp\":\"germination-experiment.csv\"\n",
    "    \n",
    "}\n",
    "# add dict for the volo files\n",
    "\n",
    "# dict for the Welten-Sutter map reference files, downloaded from here: https://www.infoflora.ch/de/daten/artenliste-welten-sutter.html\n",
    "# all observations included in this report were conducted within one of these geographic boundaries\n",
    "ws_lists ={\n",
    "    \"151\":\"AtlasWS_151_Biel.csv\",\n",
    "    \"252\":\"AtlasWS_252_Erlach.csv\",\n",
    "    \"300\":\"AtlasWS_300_Aarberg.csv\",\n",
    "    \"301\":\"AtlasWS_301_Bueren.csv\",\n",
    "    \"154\":\"AtlasWS_154_Grenchen.csv\",\n",
    "    \"572\":\"AtlasWS_572_Beatenberg.csv\",\n",
    "    \"573\":\"AtlasWS_573_Interlaken.csv\",\n",
    "    \"226\":\"AtlasWS_226_Estavayer.csv\",\n",
    "    \"251\":\"AtlasWS_251_BernWest.csv\",\n",
    "    \"145\":\"AtlasWS_145_LesRangiers.csv\"\n",
    "}\n",
    "\n",
    "# housekeeping: 585220 is separated by \",\" not \";\" like the rest of the data sources\n",
    "df = pd.read_csv(\"resources/atlas5x5/Atlas5x5_585_220.csv\", sep = \",\", encoding=\"utf-16\")\n",
    "df.to_csv('resources/atlas5x5/Atlas5x5_585_220_1.csv', sep=';', encoding = \"utf-16\", index = False)\n",
    "\n",
    "# 5X5 Flora helvitca lists\n",
    "fx_lists = {\n",
    "    \"585215\":\"Atlas5x5_585_215.csv\", # Ipsach, Bielersee\n",
    "    \"585220\":\"Atlas5x5_585_220_1.csv\", # Biel Stadt, Suze / Bielersee\n",
    "    \"580220\":\"Atlas5x5_580_220.csv\", # Biel Mett, Suze\n",
    "    \"580215\":\"Atlas5x5_580_215.csv\", # Port, Nidau-Bueren Kanal\n",
    "    \"625165\":\"Atlas5x5_625_165.csv\", # Untersee, Thunersee\n",
    "    \"625170\":\"Atlas5x5_625_170.csv\", # Sundlauenen, Thunersee\n",
    "    \"550185\":\"Atlas5x5_550_185.csv\", # Estavayer, Lac de Neuchatel\n",
    "    \"575210\":\"Atlas5x5_575_210.csv\", # Leuecherz, Bielersee\n",
    "    \"600200\":\"Atlas5x5_600_200.csv\", # Bern west, Aare\n",
    "    \"575245\":\"Atlas5x5_575_245.csv\", # Saint-Ursanne, Aare\n",
    "    \"545180\":\"Atlas5x5_545_180.csv\",\n",
    "    \"575215\":\"Atlas5x5_575_215.csv\"\n",
    "}\n",
    "\n",
    "# convenience method to gather up all the files:\n",
    "def get_the_data(file_exts, a_dir, methods, this_method=\"csv\", myencoding=None):\n",
    "    wiw = {}\n",
    "    for k,v in file_exts.items():\n",
    "        if myencoding == None:\n",
    "            wiw.update({k:methods[this_method](F\"{a_dir}{v}\")})            \n",
    "        else:\n",
    "            wiw.update({k:methods[this_method](F\"{a_dir}{v}\",sep = \";\", encoding=myencoding)})\n",
    "    return wiw\n",
    "\n",
    "# use the get_the_data method to collect these files\n",
    "data_and_keys = get_the_data(d_files, data_2020, my_data_methods, this_method=\"csv\")\n",
    "watch_lists = get_the_data(w_lists, flora_h, my_data_methods, this_method=\"csv\")\n",
    "\n",
    "# Why are we usig utf-16 here? Don't we lose some options later on?\n",
    "welt_sut =  get_the_data(ws_lists, flora_h_ws, my_data_methods, this_method=\"csv\", myencoding = \"utf-16\" )\n",
    "fivex =  get_the_data(fx_lists, flora_h_55, my_data_methods, this_method=\"csv\", myencoding = \"utf-16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-multimedia",
   "metadata": {},
   "source": [
    "## The Species columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-program",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that each data set has the column \"species\", with the value species:\n",
    "watch_lists[\"list_2014\"][\"species\"] = watch_lists[\"list_2014\"].Latin\n",
    "watch_lists[\"under_sampled\"][\"species\"] = watch_lists[\"under_sampled\"].taxon\n",
    "watch_lists[\"red_list\"][\"species\"] = watch_lists[\"red_list\"].scientific_name\n",
    "watch_lists[\"cert_list\"][\"species\"] = watch_lists[\"cert_list\"][\"Short Name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-cancer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a species slug (genus-species) to link data from across the survey and reference files. \n",
    "# This is necessary as some species columns have only \"Genus species\", some include subspecies, and some include the taxonomic reference.\n",
    "\n",
    "# function to make the species slugs\n",
    "def to_species_slug(x):\n",
    "    try: \n",
    "        int_data = x.split()\n",
    "        data = int_data[:2]\n",
    "        data = \"-\".join(data)\n",
    "        data = data.lower()\n",
    "    except:\n",
    "        data = \"none\"\n",
    "    return data\n",
    "\n",
    "# create a new column to hold the slug\n",
    "for element in [fivex, welt_sut, watch_lists]:\n",
    "    for the_data in element:\n",
    "        element[the_data]['species_slug'] = 'none'\n",
    "\n",
    "# make the species slug for all reference files\n",
    "for element in [fivex, welt_sut, watch_lists]:\n",
    "    for the_data in element:\n",
    "        element[the_data]['species_slug'] = element[the_data].species.map(lambda x: to_species_slug(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceived-parade",
   "metadata": {},
   "source": [
    "## The _map_ column and the _spec\\_map_ columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-volleyball",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add identifying columns to the reference datasets\n",
    "\n",
    "# add a column to identify the map source for the geographic data:\n",
    "for element in [fivex, welt_sut]:\n",
    "    for the_data in element:\n",
    "        element[the_data]['map'] = the_data\n",
    "        element[the_data]['spec_map'] = list(zip(element[the_data].species_slug,element[the_data].map))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-median",
   "metadata": {},
   "outputs": [],
   "source": [
    "fivex.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-wesley",
   "metadata": {},
   "outputs": [],
   "source": [
    "fivex['585215'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biblical-position",
   "metadata": {},
   "source": [
    "## The _watch\\_list_ column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amino-islam",
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in [watch_lists]:\n",
    "    for the_data in element:\n",
    "        if the_data == \"list_2014\":\n",
    "            element[the_data]['watch_list'] = element[the_data][the_data]\n",
    "        else:\n",
    "            element[the_data]['watch_list'] = the_data\n",
    "\n",
    "# housekeeping: fill in nan values in the watchlist and certification list reference files.\n",
    "fill_nans = watch_lists[\"list_2014\"].copy()\n",
    "fill_nans = fill_nans.fillna(0)\n",
    "watch_lists.update({\"list_2014\":fill_nans[fill_nans.watch_list != 0]})\n",
    "\n",
    "fill_nans = watch_lists[\"cert_list\"].copy()\n",
    "fill_nans = fill_nans.fillna(0)\n",
    "watch_lists.update({\"cert_list\":fill_nans[fill_nans.watch_list != 0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-forty",
   "metadata": {},
   "outputs": [],
   "source": [
    "watch_lists.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criminal-workstation",
   "metadata": {},
   "outputs": [],
   "source": [
    "watch_lists['red_list'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-culture",
   "metadata": {},
   "source": [
    "### Species name and observations: harmonizing taxonomy\n",
    "\n",
    "The genus-species nomenclature will be used to group observations.\n",
    "\n",
    "All observations will be classified according to that standard. As a result subspecies will be folded in with the parent species. This is a reflection of the survey method and the expectation of reasonable results, not a prioritization of importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interesting-progressive",
   "metadata": {},
   "outputs": [],
   "source": [
    "def account_for_subspecies(an_array, a_dict):\n",
    "    for element in an_array:\n",
    "        try:\n",
    "            a_dict[element[0]].append(element[1])\n",
    "        except:\n",
    "            a_dict[element[0]] = [element[1]]\n",
    "    return a_dict\n",
    "a_dict ={}\n",
    "\n",
    "for element in [fivex, welt_sut, watch_lists]:\n",
    "    for label in element:\n",
    "        # use this data frame\n",
    "        som_data = element[label].copy()\n",
    "        \n",
    "        # group by species slug and count the number of unique species values\n",
    "        c_s_p_s = som_data.groupby('species_slug', as_index=False).species.nunique()        \n",
    "        \n",
    "        # just the records with more than one species value\n",
    "        m_t_one = c_s_p_s[c_s_p_s.species > 1].species_slug\n",
    "        \n",
    "        # pair the species_slug to the species name:\n",
    "        mto = som_data.loc[som_data.species_slug.isin(m_t_one)][['species_slug', 'species']].copy().to_numpy()\n",
    "        \n",
    "        # update the dict\n",
    "        account_for_subspecies(mto, a_dict)\n",
    "\n",
    "# the species_slugs that account for more than one sub species\n",
    "sub_species_accounted =  {k:list(set(v)) for k,v in a_dict.items()}\n",
    "\n",
    "# the species_slugs\n",
    "gs_parent = list(sub_species_accounted.keys())\n",
    "\n",
    "# the number of species_slugs\n",
    "number_of_gs = len(gs_parent)\n",
    "\n",
    "# the the number of sub species accounted for\n",
    "number_of_ss = sum([len(v) for k,v in sub_species_accounted.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-locator",
   "metadata": {},
   "source": [
    "## Determine wether or not a species was detected within a geographic limit\n",
    "\n",
    "The territory is divided into different segments. Flora-helvitica and WS maps have different geographic bounds. Here the presence or not of a species within the confines of one of the different boundaries is determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-petersburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all the observations from flora helvitaca and the WS into one df\n",
    "fx = pd.concat([v[['species_slug', 'map', 'spec_map']] for k,v in fivex.items()])\n",
    "wsx = pd.concat([v[['species_slug', 'map', 'spec_map']] for k,v in welt_sut.items()])\n",
    "f_w_obs = pd.concat([fx, wsx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36940cac-8e2c-4959-929b-c75968b313ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_w_obs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e820fa-20aa-41b5-b20b-d8904d797ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacedict = {\n",
    "#     'verbanum bonariensis ':'verbena bonariensis',\n",
    "#     'medicago varia':'medicago sativa',\n",
    "#     \"oenothera\":\"oenothera biennis\",\n",
    "#     \"geranium pratens\":\"geranium pratense\",\n",
    "#     \"oenothera biennis \": \"oenothera biennis\",\n",
    "#     \"oenothera biennis agg.\": \"oenothera biennis\",\n",
    "#     \"solidalgo canadensis\": \"solidago canadensis\",\n",
    "#     \"verbascum lynchitis\":\"verbascum lychnitis\",\n",
    "#     \"verbascum negris\":\"verbascum nigrum\",\n",
    "#     \"securigea varia\": \"securigera varia\",\n",
    "#     \"melilotus officianalis\": \"melilotus officinalis\",\n",
    "#     \"knautia maxima\": \"knautia dipsacifolia\",\n",
    "#     \"hieracium aurantiacum\":\"pilosella aurantiaca\",\n",
    "#     \"sysimbrium officinale\":\"sisymbrium officinale\",\n",
    "#     \"geranium robertanium\":\"geranium robertianum\",\n",
    "#     \"mycelis muralis\": \"lactuca muralis\",\n",
    "#     \"calamintha-nepeta\":\"clinopodium nepeta\",\n",
    "#     \"polygonum-persicaria\":\"persicaria maculosa\",\n",
    "#     \"sorbus-aria\":\"aria edulis\",\n",
    "#     \"taraxacum\": \"taraxacum officinale\",\n",
    "#     \"jacobaea vulgaris\" : \"senecio jacobaea\",\n",
    "#     \"erigeron canadensis\" : \"conyza canadensis\",\n",
    "#     \"rorippa islandica\" : \"rorippa palustris\",\n",
    "#     \"malus sylvestris\" : \"malus domestica\",\n",
    "#     \"hylotelephium telephium\" : \"sedum telephium\",\n",
    "#     \"lactuca muralis\": \"mycelis muralis\",\n",
    "#     \"chaenorhinum minus\": \"chaenorrhinum minus\",\n",
    "#     \"erigeron canadensis\": \"conzya canadensis\",\n",
    "#     \"erigeron canadensis\": \"conzya canadensis\",\n",
    "#     \"borkhausenia intermedia\": \"scandosorbus intermedia\",\n",
    "#     \"centaurea nigra\" : \"centaurea jacea\"\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2ef6b1-a02c-43b7-9248-f7eb016d4418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_w_obs['ns'] = f_w_obs.species_slug\n",
    "# def replace_this(x,a_dict):\n",
    "#     if x in a_dict.keys():\n",
    "#         data=a_dict[x]\n",
    "#     else:\n",
    "#         data = x\n",
    "#     return data\n",
    "# f_w_obs['ns'] = f_w_obs.ns.map(lambda x: replace_this(x, replacedict))\n",
    "# f_w_obs['species_slug'] = f_w_obs.ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-jesus",
   "metadata": {},
   "outputs": [],
   "source": [
    "astring = F\"\"\"\n",
    "There are {len(f_w_obs.map.unique())} different map boundaries in this study\n",
    "\"\"\"\n",
    "md(astring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e810772-748d-4815-8655-c79a53aea479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = f_w_obs.set_index('species_slug')\n",
    "# a.loc['bryonia-dioica']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-welcome",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather up the map names\n",
    "m_ap_columns = f_w_obs.map.unique()\n",
    "\n",
    "# create a column for each map, indicate\n",
    "for col in m_ap_columns:\n",
    "    \n",
    "    f_w_obs[col] = f_w_obs['map'] == col\n",
    "\n",
    "obs_map =f_w_obs.groupby(['species_slug']).sum()\n",
    "\n",
    "# human readable column names need to be introduced here or a dict to rename\n",
    "obs_map.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f20d736-f344-4796-ba70-ae8add44c469",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_map.loc['bryonia-dioica']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-induction",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_file_name = \"species_map_located.csv\"\n",
    "obs_map.to_csv(F\"resources/survey-data/{a_file_name}\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superb-resistance",
   "metadata": {},
   "source": [
    "### Key the species to the different maps it was identified in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-marks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #exports the dictionary to a .json file\n",
    "# nt = obs_map['maps'].to_dict()\n",
    "\n",
    "# ntx = {k:list(v) for k,v in nt.items()}\n",
    "\n",
    "# with open(F\"{here}/output/ws_list.json\",\"w\") as afile:\n",
    "#     json.dump(ntx,afile)\n",
    "    \n",
    "# print(F\"\\nWhich maps 'trifolium-incarnatum' were found in? indifferent of subspecies?:\\n\\n{ntx['trifolium-incarnatum']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-economics",
   "metadata": {},
   "source": [
    "## The _species\\_slug_ column\n",
    "\n",
    "A coder friendly way to find the species but still maintain the proper nomenclature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powerful-picnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format scientific name in the sample dataset\n",
    "samples = data_and_keys['surveys'].copy()\n",
    "\n",
    "samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cb7790-181e-4d9b-a3a8-64b5e60f8f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of replacement values that are incorrect\n",
    "replacedict = {\n",
    "    'verbanum bonariensis ':'verbena bonariensis',\n",
    "    'medicago varia':'medicago sativa',\n",
    "    \"oenothera\":\"oenothera biennis\",\n",
    "    \"geranium pratens\":\"geranium pratense\",\n",
    "    \"oenothera biennis \": \"oenothera biennis\",\n",
    "    \"oenothera biennis agg.\": \"oenothera biennis\",\n",
    "    \"solidalgo canadensis\": \"solidago canadensis\",\n",
    "    \"verbascum lynchitis\":\"verbascum lychnitis\",\n",
    "    \"verbascum negris\":\"verbascum nigrum\",\n",
    "    \"securigea varia\": \"securigera varia\",\n",
    "    \"melilotus officianalis\": \"melilotus officinalis\",\n",
    "    \"knautia maxima\": \"knautia dipsacifolia\",\n",
    "    \"hieracium aurantiacum\":\"pilosella aurantiaca\",\n",
    "    \"sysimbrium officinale\":\"sisymbrium officinale\",\n",
    "    \"geranium robertanium\":\"geranium robertianum\",\n",
    "    \"mycelis muralis\": \"lactuca muralis\",\n",
    "    \"calamintha-nepeta\":\"clinopodium nepeta\",\n",
    "    \"polygonum-persicaria\":\"persicaria maculosa\",\n",
    "    \"sorbus-aria\":\"aria edulis\",\n",
    "    \"taraxacum\": \"taraxacum officinale\",\n",
    "    \"jacobaea vulgaris\" : \"senecio jacobaea\",\n",
    "    \"erigeron canadensis\" : \"conyza canadensis\",\n",
    "    \"rorippa islandica\" : \"rorippa palustris\",\n",
    "    \"malus sylvestris\" : \"malus domestica\",\n",
    "    \"hylotelephium telephium\" : \"sedum telephium\",\n",
    "    \"lactuca muralis\": \"mycelis muralis\",\n",
    "    \"chaenorhinum minus\": \"chaenorrhinum minus\",\n",
    "    \"erigeron canadensis\": \"conzya canadensis\",\n",
    "    \"erigeron canadensis\": \"conzya canadensis\",\n",
    "    \"borkhausenia intermedia\": \"scandosorbus intermedia\",\n",
    "    \"centaurea nigra\" : \"centaurea jacea\"\n",
    "}\n",
    "\n",
    "# function to assign the correct value of the key is in the samples dictionary.\n",
    "def new_func(x,keys):\n",
    "    try:\n",
    "        data = keys[x]\n",
    "    except:\n",
    "        data = x\n",
    "    return data\n",
    "\n",
    "# apply the funtion to a copy of the surveys data set.\n",
    "samples[\"species2\"] = samples.sci.map(lambda x: new_func(x, replacedict))\n",
    "samples[\"species_slug\"] = samples.species2.map(lambda x: to_species_slug(x))\n",
    "\n",
    "# update the surveys dataset.\n",
    "data_and_keys.update({'surveys':samples})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-developer",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_and_keys[\"surveys\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-percentage",
   "metadata": {},
   "source": [
    "## Format date column to ISO standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-equation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format dates in the sample dataset\n",
    "\n",
    "# function converts DD.MM.YYYY format to YYYY-MM-DD format, ignores if already in YYYY-MM-DD format\n",
    "# Do we even need this any more ? Looking at the cell above it appears the dates have been fixed at the source\n",
    "def change_string(x):\n",
    "    try:\n",
    "        s_data = x.split('.')\n",
    "        data = s_data[::-1]\n",
    "        data = \"-\".join(data)\n",
    "    except:\n",
    "        print(\"no luck\")\n",
    "        data = x\n",
    "    \n",
    "    return data\n",
    "\n",
    "# applies the function to a column in the samples data frame\n",
    "samples['new_date'] = samples.date.map(lambda x: change_string(x))\n",
    "\n",
    "# function makes a timestamp out of the YYYY-MM-DD string.\n",
    "# def make_timestamp(x):\n",
    "#     try:        \n",
    "#         data = dt.datetime.strptime(x, \"%Y-%m-%d\")        \n",
    "#     except:        \n",
    "#         data = 'no luck'\n",
    "    \n",
    "#     return data\n",
    "\n",
    "# run the make_timestamp function and store the results in the samples dataframe.\n",
    "# samples['stamp_date'] = samples.new_date.map(lambda x: make_timestamp(x))\n",
    "# samples['date'] = samples.stamp_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-receiver",
   "metadata": {},
   "source": [
    "## The _place1_ column\n",
    "\n",
    "A coder friendly way to find the species but still maintain the proper nomenclature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-toddler",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make place name slugs in the sample dataset\n",
    "#  change_place function turns \"place names 1\" into place-names-1\n",
    "def change_place(x):\n",
    "    data = x.split(\" \")\n",
    "    data = \"-\".join(data)\n",
    "    return data\n",
    "\n",
    "samples[\"place_slug\"] = samples.place.map(lambda x: change_place(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "duplicate-device",
   "metadata": {},
   "source": [
    "## Housekeeping and export of processed survey data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81901b7-1fdf-4f5e-a074-acf6b491f8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up redundant and unused column names\n",
    "# rename the place_slug column\n",
    "\n",
    "samples.rename(columns= {'place1':'place_slug', 'new_date':'str_date'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06d9454-672f-43a0-8561-11cc9bb0a38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instructional-stupid",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the unnecessary columns for this analysis\n",
    "samples.drop(['time',  'sci', 'name', 'place', 'species2'] , inplace=True, axis=1)\n",
    "\n",
    "samples['loc_date'] = list(zip(samples.place_slug, samples.date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ideal-antibody",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export samples to a .csv for later use\n",
    "samples.to_csv(F\"resources/preprocessed/hd_samples_2020.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intensive-logging",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-married",
   "metadata": {},
   "source": [
    "Subspecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-intro",
   "metadata": {},
   "outputs": [],
   "source": [
    "def account_for_subspecies(an_array, a_dict):\n",
    "    for element in an_array:\n",
    "        try:\n",
    "            a_dict[element[0]].append(element[1])\n",
    "        except:\n",
    "            a_dict[element[0]] = [element[1]]\n",
    "    return a_dict\n",
    "a_dict ={}\n",
    "\n",
    "for element in [fivex, welt_sut, watch_lists]:\n",
    "    for label in element:\n",
    "        # use this data frame\n",
    "        som_data = element[label].copy()\n",
    "        \n",
    "        # group by species slug and count the number of unique species values\n",
    "        c_s_p_s = som_data.groupby('species_slug', as_index=False).species.nunique()        \n",
    "        \n",
    "        # just the records with more than one species value\n",
    "        m_t_one = c_s_p_s[c_s_p_s.species > 1].species_slug\n",
    "        \n",
    "        # pair the species_slug to the species name:\n",
    "        mto = som_data.loc[som_data.species_slug.isin(m_t_one)][['species_slug', 'species']].copy().to_numpy()\n",
    "        \n",
    "        # update the dict\n",
    "        account_for_subspecies(mto, a_dict)\n",
    "\n",
    "# the species_slugs that account for more than one sub species\n",
    "sub_species_accounted =  {k:list(set(v)) for k,v in a_dict.items()}\n",
    "\n",
    "# the species_slugs\n",
    "gs_parent = list(sub_species_accounted.keys())\n",
    "\n",
    "# the number of species_slugs\n",
    "number_of_gs = len(gs_parent)\n",
    "\n",
    "# the the number of sub species accounted for\n",
    "number_of_ss = sum([len(v) for k,v in sub_species_accounted.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-involvement",
   "metadata": {},
   "source": [
    "### Key the watch lists to genus-species\n",
    "\n",
    "Identify which species are on which watch list. Produce a boolean matrix with species_slug as index and watch list name for the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-meter",
   "metadata": {},
   "outputs": [],
   "source": [
    "the_lists = list(watch_lists.keys())\n",
    "\n",
    "keep = []\n",
    "\n",
    "for element in the_lists:\n",
    "    a = watch_lists[element][['species_slug','watch_list']]\n",
    "    keep.append(a)\n",
    "wl_species = pd.concat(keep)\n",
    "m_ap_columns = wl_species.watch_list.unique()\n",
    "\n",
    "for col in m_ap_columns:    \n",
    "    wl_species[col] = wl_species['watch_list'] == col\n",
    "\n",
    "wl_sp_map =wl_species.groupby(['species_slug']).sum()\n",
    "\n",
    "wl_sp_map['lists'] = wl_sp_map.index.map(lambda x: wl_species[wl_species.species_slug == x]['watch_list'].unique())\n",
    "\n",
    "a_filename = \"species_keyed_to_watch_list.csv\"\n",
    "wl_sp_map.reset_index().to_csv(F\"resources/{a_filename}\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greenhouse-moisture",
   "metadata": {},
   "outputs": [],
   "source": [
    "wl_sp_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-sweet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nt = wl_sp_map['lists'].to_dict()\n",
    "\n",
    "# ntx = {k:list(v) for k,v in nt.items()}\n",
    "\n",
    "# with open(F\"{here}/output/flora_list.json\",\"w\") as afile:\n",
    "#     json.dump(ntx,afile)\n",
    "# wl_sp_map.to_csv(F\"{here}/resources/preprocessed/aggregated_list.csv\",index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5a1226-19dd-4d69-8249-eb821d83ca3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
